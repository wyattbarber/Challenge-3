#ifndef _LOSS_HPP
#define _LOSS_HPP

#include <Eigen/Dense>
#include <unsupported/Eigen/CXX11/Tensor>

namespace loss
{
    /** Basic interface for a loss function
     * 
     * @tparam T Scalar type for evaluating the loss function
     */
    template<typename T>
    class Loss
    {
        public:
        /** Evaluates the loss gradient.
         * 
         * Returns the partial gradient of the loss with respect
         * to each predicted value. The net loss is stored in the 
         * given `loss` parameter.
         * 
         * @param predicted Data generated by a model
         * @param actual Ground truth data
         * @param loss Optional reference to variable that net loss will be stored in
        */
        virtual Eigen::Vector<T, Eigen::Dynamic> grad(Eigen::Vector<T, Eigen::Dynamic>& predicted, Eigen::Vector<T, Eigen::Dynamic>& actual, T& loss) = 0;
        virtual Eigen::Tensor<T, 3> grad(Eigen::Tensor<T, 3>& predicted, Eigen::Tensor<T, 3>& actual, T& loss) = 0;
        
        template<typename X>
        X grad(X& predicted, X& actual)
        {
            T t;
            return grad(predicted, actual, t);
        }

        protected:

        /** Helper for converting Eigen vectors, matrices, and tensors to common type.
         * 
         * @param data Tensor, Vector, or Matrix to convert
         * @return Array containing flattened data 
        */
        template<typename X>
        static Eigen::Array<T, Eigen::Dynamic, 1> make_array(X& data)
        {
            return Eigen::Map<Eigen::Array<T, Eigen::Dynamic, 1>>(
                data.data(), data.size()
            );
        }
    };


    template<typename T>
    class LossTrampoline : public Loss<T>
    {
        protected:
            typedef Eigen::Vector<T,Eigen::Dynamic> ret1;
            typedef Eigen::Tensor<T,3> ret2;

        public:
            using Loss<T>::Loss;


            Eigen::Vector<T,Eigen::Dynamic> grad(Eigen::Vector<T, Eigen::Dynamic>& predicted, Eigen::Vector<T, Eigen::Dynamic>& actual, T& loss) override
            {
                PYBIND11_OVERLOAD_PURE(
                    ret1, /* Return type */
                    Loss<T>,      /* Parent class */
                    grad,        /* Name of function in C++ (must match Python name) */
                    predicted, actual, loss      /* Argument(s) */
                );
            }

            Eigen::Tensor<T,3> grad(Eigen::Tensor<T,3>& predicted, Eigen::Tensor<T,3>& actual, T& loss) override
            {
                PYBIND11_OVERLOAD_PURE(
                    ret2, /* Return type */
                    Loss<T>,      /* Parent class */
                    grad,        /* Name of function in C++ (must match Python name) */
                    predicted, actual, loss      /* Argument(s) */
                );
            }

    };
}


#endif